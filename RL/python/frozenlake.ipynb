{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a938a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random, time\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad27387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(V):\n",
    " # reshape value function\n",
    " V_sq = np.reshape(V, (4,4))\n",
    " # plot the state-value function\n",
    " fig = plt.figure(figsize=(6, 6))\n",
    " ax = fig.add_subplot(111)\n",
    " im = ax.imshow(V_sq, cmap='cool')\n",
    " for (j,i),label in np.ndenumerate(V_sq):\n",
    "  ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n",
    " plt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    " plt.title('State-Value Function')\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c663378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenLakeWrapper(gym.Env):\n",
    "    def __init__(self, desc=None, map_name=\"4x4\", is_slippery=True, gamma: float=1.0):\n",
    "        self.__env = gym.make('FrozenLake-v0', desc=desc, map_name=map_name, is_slippery=is_slippery)\n",
    "        self.observation_space = self.__env.observation_space\n",
    "        self.action_space = self.__env.action_space\n",
    "        self.reward_range = self.__env.reward_range\n",
    "        self.__gamma = gamma\n",
    "\n",
    "    def reset(self):\n",
    "        return self.__env.reset()\n",
    "\n",
    "    def step(self, action):\n",
    "        return self.__env.step(action)\n",
    "\n",
    "    def execute(self, state: int, action: int):\n",
    "        next_state, reward, _, _ = self.__env.step(action)\n",
    "        return next_state, reward\n",
    "\n",
    "    def get_actions(self, state: int):\n",
    "        return [action for action in range(self.action_space.n) if self.__env.P[state][action]]\n",
    "\n",
    "    def get_transitions(self, state: int, action: int):\n",
    "        return [(x[1],x[0]) for x in self.__env.P[state][action]]\n",
    "\n",
    "    def get_initial_state(self):\n",
    "        return 0\n",
    "\n",
    "    def get_discount_factor(self):\n",
    "        return self.__gamma\n",
    "\n",
    "    def is_terminal(self, state: int):\n",
    "        return self.__env.env.desc.flatten()[state] in b'GH'\n",
    "\n",
    "    def close(self):\n",
    "        return self.__env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5122f6f5",
   "metadata": {},
   "source": [
    "# Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda13f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def simulate_step(env, state: int, action: int=None):\n",
    "#  # Save the current state\n",
    "#  original_state = env.env.s\n",
    "\n",
    "#  # Switch to given state\n",
    "#  env.env.s = state\n",
    "\n",
    "#  # Choose an action\n",
    "#  action = action if action else env.action_space.sample()\n",
    "\n",
    "#  # Simulate the step without modifying the original environment's state\n",
    "#  next_state, reward, done, _ = env.env.step(action)\n",
    "\n",
    "#  # Restore the original state\n",
    "#  env.env.s = original_state\n",
    "\n",
    "#  return next_state, reward, done\n",
    "\n",
    "class Node:\n",
    "\n",
    " # Record a unique node id to distinguish duplicated states\n",
    " next_node_id = 0\n",
    "\n",
    " # Records the number of times states have been visited\n",
    " visits = defaultdict(lambda: 0)\n",
    "\n",
    " def __init__(self, env, Q, state=0, reward=0.0, action=None, parent=None, gamma=1.0):\n",
    "  self.env    = env\n",
    "  self.parent = parent\n",
    "  self.state  = state\n",
    "  self.reward = reward\n",
    "  self.action = action\n",
    "  self.gamma  = gamma\n",
    "  self.children = {}\n",
    "  self.id = Node.next_node_id\n",
    "  self.Q  = Q\n",
    "\n",
    "  Node.next_node_id += 1\n",
    "\n",
    " def get_nA(self):\n",
    "  # All actions are possible for a given state\n",
    "  return self.env.nA\n",
    "\n",
    " def get_actions(self):\n",
    "  # All actions are possible for a given state\n",
    "  return set(range(self.env.nA))\n",
    "\n",
    " def get_visits(self):\n",
    "  return Node.visits[self.state]\n",
    "\n",
    " def is_fully_expanded(self):\n",
    "   return True if self.get_nA() == len(self.children) else False\n",
    "\n",
    " def is_terminal(self, state: int=None):\n",
    "  return self.env.desc.flatten()[state] in b'GH'\n",
    "\n",
    " \"\"\" Select a node that is not fully expanded \"\"\"\n",
    "\n",
    " def select(self):\n",
    "  node = self\n",
    "  while node.is_fully_expanded():\n",
    "   if not node.children or node.is_terminal():\n",
    "    return node\n",
    "\n",
    "   actions = list(node.children.keys())\n",
    "   action  = random.choice(actions)\n",
    "   node    = random.choice(node.children[action])[0]\n",
    "\n",
    "  return node\n",
    "\n",
    " \"\"\" Expand a node if it is not a terminal node \"\"\"\n",
    "\n",
    " def expand(self):\n",
    "\n",
    "  if self.is_terminal():\n",
    "   return self\n",
    "\n",
    "  # Choose at random an action that has not been expanded\n",
    "  actions = list(self.get_actions() - self.children.keys())\n",
    "  action  = random.choice(actions)\n",
    "\n",
    "  # Synchronize the state of the environment with the state of the node\n",
    "  self.env.reset()\n",
    "  self.env.env.s = self.state\n",
    "  # Execute the action\n",
    "  next_state, reward, done, info = self.env.env.step(action)\n",
    "\n",
    "  # Initialize new child node\n",
    "  child = Node(self.env, self.Q, next_state, reward, action, self, self.gamma)\n",
    "\n",
    "  # Compute the correct transition probability\n",
    "  transition = defaultdict(float)\n",
    "  for prob, next_s, *rest in env.P[self.state][action]:\n",
    "   transition[next_s] = prob\n",
    "\n",
    "  # self.children[action] += [(child, info['prob'])]\n",
    "  self.children[action] = [(child, transition[next_state])]\n",
    "  return child\n",
    "\n",
    " \"\"\" Simulate until a terminal state \"\"\"\n",
    "\n",
    " def simulate(self):\n",
    "\n",
    "  # Set the inner state of the environment to the node state\n",
    "  self.env.reset()\n",
    "  self.env.env.s = self.state\n",
    "\n",
    "  # Initialization\n",
    "  reward = 0.0\n",
    "  depth  = 0\n",
    "  done   = False\n",
    "\n",
    "  # Simulate reward\n",
    "  while not done:\n",
    "\n",
    "   # Choose an action to execute\n",
    "   action = self.env.action_space.sample()\n",
    "\n",
    "   # Execute the action\n",
    "   _, r, done, _ = self.env.env.step(action)\n",
    "\n",
    "   # Discount the reward\n",
    "   reward += np.power(self.gamma, depth) * r\n",
    "   depth  += 1\n",
    "\n",
    "  return reward\n",
    "\n",
    " \"\"\" Backpropogate the reward back to the parent node \"\"\"\n",
    " def backpropagate(self, reward, child):\n",
    "  action = child.action\n",
    "\n",
    "  node   = self\n",
    "  while node is not None:\n",
    "   Node.visits[node.state] += 1\n",
    "   Node.visits[(node.state, action)] += 1\n",
    "\n",
    "   Nsa   = Node.visits[(self.state, action)]\n",
    "   Qsa   = self.Q[(self.state, action)]\n",
    "   delta = 1.0 * (reward - Qsa) / Nsa\n",
    "\n",
    "   self.Q[(self.state, action)] += delta\n",
    "\n",
    "   node = node.parent\n",
    "   if node is not None:\n",
    "    node.reward += reward\n",
    "\n",
    "\n",
    "#  def select(self):\n",
    "#   if not self.is_fully_expanded() or self.is_terminal():\n",
    "#    return self\n",
    "#   else:\n",
    "#    actions = list(self.children.keys())\n",
    "#    action  = random.choice(actions)\n",
    "#    return self.get_outcome_child(action).select()\n",
    "\n",
    "#  def expand(self):\n",
    "\n",
    "#   if self.is_terminal():\n",
    "#    return self\n",
    "\n",
    "#   actions = list(self.get_actions() - self.children.keys())\n",
    "#   action  = random.choice(actions)\n",
    "#   self.children[action] = []\n",
    "\n",
    "#   return self.get_outcome_child(action)\n",
    "\n",
    "#  def back_propagate(self, reward, child):\n",
    "#   action = child.action\n",
    "\n",
    "#   Node.visits[self.state] += 1\n",
    "#   Node.visits[(self.state, action)] += 1\n",
    "\n",
    "#   Nsa   = Node.visits[(self.state, action)]\n",
    "#   Qsa   = self.Q[(self.state, action)]\n",
    "#   delta = 1.0 * (reward - Qsa) / Nsa\n",
    "\n",
    "#   self.Q[(self.state, action)] += delta\n",
    "\n",
    "#   if self.parent != None:\n",
    "#    self.parent.back_propagate(self.reward + reward, self)\n",
    "\n",
    "#  \"\"\" Simulate the outcome of an action, and return the child node \"\"\"\n",
    "\n",
    "#  def get_outcome_child(self, action):\n",
    "\n",
    "#   self.env.reset()\n",
    "#   self.env.env.s = self.state\n",
    "#   # Choose one outcome based on transition probabilities\n",
    "#   next_state, reward, done, _ = self.env.env.step(action)\n",
    "\n",
    "#   # Find the corresponding state and return if this already exists\n",
    "#   for child, _ in self.children[action]:\n",
    "#    if next_state == child.state:\n",
    "#     return child\n",
    "\n",
    "#   # This outcome has not occured from this state-action pair previously\n",
    "#   new_child = Node(self.env, self.Q, next_state, reward, action, self, self.gamma)\n",
    "\n",
    "#   # Find the probability of this outcome (only possible for model-based) for visualising tree\n",
    "#   probability = 0.0\n",
    "#   for probability, outcome, _, _ in self.env.P[self.state][action]:\n",
    "#    if outcome == next_state:\n",
    "#     self.children[action] += [(new_child, probability)]\n",
    "#     return new_child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82552d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200000\n",
    "rewards = np.zeros(N)\n",
    "\n",
    "Q = defaultdict(lambda: 0.0)\n",
    "env = gym.make('FrozenLake-v0', desc=None, map_name=\"4x4\", is_slippery=True)\n",
    "env.reset()\n",
    "node = Node(env, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432819c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(N):\n",
    " current = node.select()\n",
    " child   = current.expand()\n",
    " reward  = child.simulate()\n",
    " current.backpropagate(reward, child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f07bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qvec = np.zeros((env.nS, env.nA))\n",
    "for (s, a), q in Q.items():\n",
    " Qvec[s,a] = q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca2a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(np.max(Qvec, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308f05c",
   "metadata": {},
   "source": [
    "## Simulation tests for gym env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569107ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "rewards = np.zeros(N)\n",
    "\n",
    "Q = defaultdict(lambda: 0.0)\n",
    "node = Node(env, Q)\n",
    "\n",
    "for i in range(N):\n",
    " rewards[i] = node.simulate()\n",
    "\n",
    "np.sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf86b7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in Q.items():\n",
    " print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da53eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "rewards = np.zeros(N)\n",
    "\n",
    "for i in range(N):\n",
    " env.reset()\n",
    " done = False\n",
    " reward = 0.0\n",
    " state = env.env.s\n",
    " while not done:\n",
    "  action = env.action_space.sample()\n",
    "  # _, next_state, r, done = random.choice(env.P[state][action])\n",
    "  next_state, r, done, _ = env.step(action)\n",
    "  reward += r\n",
    " rewards[i] = reward\n",
    "\n",
    "np.sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1338bc85",
   "metadata": {},
   "source": [
    "# Dynamic programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc45a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake-v0', desc=None, map_name=\"4x4\", is_slippery=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe3d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_values(V):\n",
    " # reshape value function\n",
    " V_sq = np.reshape(V, (4,4))\n",
    " # plot the state-value function\n",
    " fig = plt.figure(figsize=(6, 6))\n",
    " ax = fig.add_subplot(111)\n",
    " im = ax.imshow(V_sq, cmap='cool')\n",
    " for (j,i),label in np.ndenumerate(V_sq):\n",
    "  ax.text(i, j, np.round(label, 5), ha='center', va='center', fontsize=14)\n",
    " plt.tick_params(bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    " plt.title('State-Value Function')\n",
    " plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11aea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic programming\n",
    "def dynamic_program(env, gamma=1.0, eps=1e-8):\n",
    " V = np.zeros(env.nS)\n",
    "\n",
    " while True:\n",
    "  Q     = np.zeros((env.nS, env.nA))\n",
    "  delta = 0.0\n",
    "\n",
    "  for s in range(env.nS):\n",
    "   Vs = 0.0\n",
    "\n",
    "   for a in range(env.nA):\n",
    "    for prob, next_state, reward, done in env.P[s][a]:\n",
    "     # Vs += (1.0 / env.nA) * prob * (reward + gamma * V[next_state])\n",
    "     Q[s,a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "   Vs    = np.sum((1.0 / env.nA) * Q[s,])\n",
    "   delta = max(delta, np.abs(V[s]-Vs))\n",
    "   V[s]  = Vs\n",
    "\n",
    "  if delta < eps:\n",
    "   break\n",
    "\n",
    " return V, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd8df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic_program(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e061816a",
   "metadata": {},
   "outputs": [],
   "source": [
    "V, _ = dynamic_program(env)\n",
    "# Extracting Q from V with respect to a policy\n",
    "gamma = 1.0\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "\n",
    "for s in range(env.nS):\n",
    " for a in range(env.nA):\n",
    "  for prob, next_state, reward, done in env.P[s][a]:\n",
    "   Q[s,a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy improvement\n",
    "policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "Q = np.zeros((env.nS, env.nA))\n",
    "\n",
    "for s in range(env.nS):\n",
    " for a in range(env.nA):\n",
    "  for prob, next_state, reward, done in env.P[s][a]:\n",
    "   Q[s,a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    " a_opt     = np.flatnonzero(Q[s,] == np.max(Q[s,]))\n",
    " policy[s] = np.zeros(env.nA)\n",
    " policy[s,a_opt] = 1.0 / len(a_opt)\n",
    "\n",
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27220510",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(env, policy, gamma=1.0, eps=1e-8):\n",
    " V     = np.zeros(env.nS)\n",
    "\n",
    " while True:\n",
    "  delta = 0.0\n",
    "\n",
    "  for s in range(env.nS):\n",
    "   Vs = 0.0\n",
    "\n",
    "   for a, prob_a in enumerate(policy[s]):\n",
    "    for prob, next_state, reward, done in env.P[s][a]:\n",
    "     Vs += prob_a * prob * (reward + gamma * V[next_state])\n",
    "\n",
    "   delta = max(delta, np.abs(V[s]-Vs))\n",
    "   V[s]  = Vs\n",
    "\n",
    "  if delta < eps:\n",
    "   break\n",
    "\n",
    " return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d111db",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    "V = policy_evaluation(env, random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984429f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695c4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Q from V with respect to a policy\n",
    "def policy_extraction(env, V, s, gamma=1.0):\n",
    " qs = np.zeros(env.nA)\n",
    "\n",
    " for a in range(env.nA):\n",
    "  for prob, next_state, reward, done in env.P[s][a]:\n",
    "   qs[a] += prob * (reward + gamma * V[next_state])\n",
    "\n",
    " return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01668b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "V = policy_evaluation(env, random_policy)\n",
    "policy_extraction(env, V, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d459d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=1.0):\n",
    " policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "\n",
    " for s in range(env.nS):\n",
    "  qs    = policy_extraction(env, V, s, gamma)\n",
    "  a_opt = np.flatnonzero(qs == np.max(qs))\n",
    "  # Initialize policy vector\n",
    "  policy[s] = np.zeros(env.nA)\n",
    "  # Uniform probability of optimal action(s)\n",
    "  policy[s,a_opt] = 1.0 / len(a_opt)\n",
    "\n",
    " return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2a4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_improvement(env, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd75f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(env, gamma=1.0, eps=1e-8):\n",
    " pi = np.ones([env.nS, env.nA]) / env.nA\n",
    "\n",
    " while True:\n",
    "  V     = policy_evaluation(env, pi, gamma, eps)\n",
    "  newpi = policy_improvement(env, V, gamma)\n",
    "\n",
    "  if (newpi == pi).all():\n",
    "   break\n",
    "\n",
    "  pi[:] = newpi\n",
    "\n",
    " return pi, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209b9a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi, V_pi = policy_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a65a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(V_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c05ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1.0, eps=1e-8):\n",
    " V = np.zeros(env.nS)\n",
    "\n",
    " while True:\n",
    "  delta = 0.0\n",
    "\n",
    "  for s in range(env.nS):\n",
    "   v     = V[s]\n",
    "   V[s]  = max(policy_extraction(env, V, s, gamma))\n",
    "   delta = max(delta, abs(V[s]-v))\n",
    "\n",
    "  if delta < eps:\n",
    "   break\n",
    "\n",
    " # policy = policy_improvement(env, V, gamma)\n",
    " # return policy, V\n",
    " return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ae970",
   "metadata": {},
   "outputs": [],
   "source": [
    "V_pi = value_iteration(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c97996",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_values(V_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d870bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQN",
   "language": "python",
   "name": "dqn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
